{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('/home/yxf118/ROS_car/vehicle_data/cnn_features.npz')\n",
    "#data = np.load('/home/yxf118/ROS_car/vehicle_data/dataset.npz')\n",
    "X_train, y_train = data['X_train'], data['y_train']\n",
    "X_test, y_test = data['X_test'], data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "batch_size = 200\n",
    "epochs = 40\n",
    "n_input = 64 # data features(64))\n",
    "n_steps = 10 # timesteps\n",
    "n_seqs = batch_size/n_steps\n",
    "n_hidden = 200 # hidden layer num\n",
    "n_layers = 2\n",
    "n_classes = 7 \n",
    "keep_prob = 0.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3025, 64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3025, 7)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.39717662,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,  11.44458485,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         4.24545288,   0.        ,   4.41768837,   0.        ,\n",
       "         7.37591171,   0.        ,   5.9482255 ,   0.        ,\n",
       "         0.        ,  18.33749771,   0.        ,   0.60286605,\n",
       "         0.        ,   0.        ,   7.91732502,  21.76796341,\n",
       "         0.85880768,  10.54665375,   9.31004524,   0.41251412,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         9.73074627,   0.        ,   0.37255639,   4.39681864,\n",
       "         0.        ,   0.        ,   0.        ,  11.11513519,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,  11.45240688,   0.        ,\n",
       "         2.02771354,   0.        ,   0.10151491,   0.        ,\n",
       "         0.        ,   4.74020338,   1.10240889,   0.        ], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.,  0.,  0.,  0.,  1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(X,Y, n_seqs, n_steps,n_input,n_classes):\n",
    "    '''\n",
    "    slice the mini-batches\n",
    "    \n",
    "    X: X_input, to be sliced\n",
    "    n_seqs: number of sequences\n",
    "    n_steps: num of steps (in time)\n",
    "    n_inputs: input features\n",
    "    n_classes: output classes\n",
    "    '''\n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = int(len(X) / batch_size)\n",
    "    # keep only integer batches\n",
    "    X = X[:int(batch_size * n_batches)]\n",
    "    Y = Y[:int(batch_size * n_batches)]\n",
    "    \n",
    "    # reshape\n",
    "    X = X.reshape((int(n_seqs), -1, n_input))\n",
    "    Y = Y.reshape((int(n_seqs), -1, n_classes))\n",
    "    \n",
    "    for n in range(0, X.shape[1], n_steps):\n",
    "        # inputs\n",
    "        x = X[:, n:n+n_steps,:]\n",
    "        # targets\n",
    "        y = np.zeros((x.shape[0],x.shape[1],n_input))\n",
    "        y = Y[:, n:n+n_steps,:]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(X_train,y_train,10,10,64,7)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_inputs(num_seqs, num_steps,n_classes):\n",
    "    '''\n",
    "    building the input layer\n",
    "    \n",
    "    num_seqs: number of sequences in every batch (1st dimension)\n",
    "    num_steps: number of time steps in each sequence (2nd dimension)\n",
    "    '''\n",
    "#    inputs = tf.placeholder(tf.float32, shape=(None, num_steps, n_input), name='inputs')\n",
    "#    targets = tf.placeholder(tf.int64, shape=(None, num_steps, n_classes), name='targets')\n",
    "    inputs = tf.placeholder(tf.float32, shape=(num_seqs, num_steps, n_input), name='inputs')\n",
    "    targets = tf.placeholder(tf.float32, shape=(num_seqs, num_steps, n_classes), name='targets')\n",
    "\n",
    "    # add the keep_prob\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    return inputs, targets, keep_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_lstm(lstm_size, num_layers, batch_size,num_steps, keep_prob):\n",
    "    ''' \n",
    "    building the lstm layer\n",
    "        \n",
    "    keep_prob: dropout keep probability\n",
    "    lstm_size: number of hidden units in lstm layer\n",
    "    num_layers: number of lstm layers\n",
    "    batch_size: batch_size\n",
    "\n",
    "    '''\n",
    "    # build a basic lstm unit\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    \n",
    "    # adding dropout\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    \n",
    "    lstm2 = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    drop2 = tf.contrib.rnn.DropoutWrapper(lstm2, output_keep_prob=keep_prob)\n",
    "\n",
    "    stack_rnn = [drop]\n",
    "    for _ in range(num_layers-1):\n",
    "        stack_rnn.append(drop2)\n",
    "\n",
    "\n",
    "    # stack (changed in TF 1.2)   \n",
    "    cell = tf.contrib.rnn.MultiRNNCell(stack_rnn, state_is_tuple = True)\n",
    "#    cell = tf.contrib.rnn.MultiRNNCell([drop for _ in range(num_layers)])\n",
    "\n",
    "    initial_state = cell.zero_state(int(batch_size/num_steps), tf.float32)\n",
    "    \n",
    "    return cell, initial_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_output(lstm_output, in_size, out_size):\n",
    "    ''' \n",
    "    building the output layer\n",
    "        \n",
    "    lstm_output: the output of the lstm layer\n",
    "    in_size: lstm layer reshaped size\n",
    "    out_size: softmax layer size\n",
    "    \n",
    "    '''\n",
    "\n",
    "    # concate lstm output according to the output sequence，\n",
    "    # [[1,2,3],[7,8,9]] tf.concat -> [1,2,3,7,8,9]\n",
    "    seq_output = tf.concat(lstm_output, axis=1) # tf.concat(concat_dim, values)\n",
    "    # reshape\n",
    "    x = tf.reshape(seq_output, [-1, in_size])\n",
    "    \n",
    "    # fully connect lstm to softmax\n",
    "    with tf.variable_scope('softmax'):\n",
    "        softmax_w = tf.Variable(tf.truncated_normal([in_size, out_size], stddev=0.1))\n",
    "        softmax_b = tf.Variable(tf.zeros(out_size))\n",
    "    \n",
    "    # compute logits\n",
    "    logits = tf.matmul(x, softmax_w) + softmax_b\n",
    "    \n",
    "    # softmax return\n",
    "    out = tf.nn.softmax(logits, name='predictions')\n",
    "    \n",
    "    return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_loss(logits, targets, lstm_size, num_classes):\n",
    "    '''\n",
    "    compute loss using logits and targets\n",
    "    \n",
    "    logits: fully connected layer output（before softmax）\n",
    "    targets: targets\n",
    "    lstm_size: lstm_size\n",
    "    num_classes: class size\n",
    "        \n",
    "    '''\n",
    "#    # One-hot coding\n",
    "#    y_one_hot = tf.one_hot(targets, num_classes)\n",
    "#    y_reshaped = tf.reshape(y_one_hot, logits.get_shape())\n",
    "#    \n",
    "#    # Softmax cross entropy loss\n",
    "#    loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "#    loss = tf.reduce_mean(loss)\n",
    "    \n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_optimizer(loss, learning_rate, grad_clip):\n",
    "    \n",
    "    tvars = tf.trainable_variables()\n",
    "    grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), grad_clip)\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "    optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "    \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_RNN:\n",
    "    \n",
    "    def __init__(self, num_classes, batch_size=100, num_steps=10, \n",
    "                       lstm_size=128, num_layers=n_layers, learning_rate=0.001, \n",
    "                       grad_clip=5, sampling=False):\n",
    "    \n",
    "        # if sampling is True，use SGD, only 1 sample\n",
    "        if sampling == True:\n",
    "            batch_size, num_steps = 1, 1\n",
    "        else:\n",
    "            batch_size, num_steps = batch_size, num_steps\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # input layer\n",
    "        self.inputs, self.targets, self.keep_prob = build_inputs(n_seqs, num_steps, num_classes)\n",
    "\n",
    "        # LSTM layer\n",
    "        cell, self.initial_state = build_lstm(lstm_size, num_layers, batch_size, n_steps,self.keep_prob)\n",
    "\n",
    "#        # one-hot coding for inputs\n",
    "#        x_one_hot = tf.one_hot(self.inputs, num_classes)\n",
    "        \n",
    "        # running the RNN\n",
    "        outputs, state = tf.nn.dynamic_rnn(cell, self.inputs, initial_state=self.initial_state)\n",
    "        self.final_state = state\n",
    "        \n",
    "        # predicting the results\n",
    "        self.prediction, self.logits = build_output(outputs, lstm_size, num_classes)\n",
    "        \n",
    "        # Loss and optimizer (with gradient clipping)\n",
    "        self.loss = build_loss(self.logits, self.targets, lstm_size, num_classes)\n",
    "        self.optimizer = build_optimizer(self.loss, learning_rate, grad_clip)\n",
    "        \n",
    "        correct_predictions = tf.equal(tf.argmax(self.prediction, 1),\n",
    "                                       tf.reshape(tf.argmax(self.targets, 2), [-1]))\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LSTM_RNN(y_train.shape[1], batch_size=batch_size, num_steps=n_steps,\n",
    "                lstm_size=n_hidden, num_layers=2, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training progress:   0%|          | 0/40 [00:00<?, ?it/s]\n",
      "Training progress:   2%|▎         | 1/40 [00:01<00:56,  1.45s/it]\n",
      "Training progress:   5%|▌         | 2/40 [00:02<00:44,  1.18s/it]\n",
      "Training progress:   8%|▊         | 3/40 [00:02<00:36,  1.00it/s]\n",
      "Training progress:  10%|█         | 4/40 [00:03<00:31,  1.16it/s]\n",
      "Training progress:  12%|█▎        | 5/40 [00:03<00:27,  1.29it/s]\n",
      "Training progress:  15%|█▌        | 6/40 [00:04<00:24,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoches: 7/40...  Training Steps: 100...  Training Loss: 0.1244...  0.0394 sec/batch\n",
      "\n",
      " Test accuracy: 0.6600... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training progress:  18%|█▊        | 7/40 [00:04<00:22,  1.47it/s]\n",
      "Training progress:  20%|██        | 8/40 [00:05<00:20,  1.54it/s]\n",
      "Training progress:  22%|██▎       | 9/40 [00:06<00:19,  1.61it/s]\n",
      "Training progress:  25%|██▌       | 10/40 [00:06<00:18,  1.64it/s]\n",
      "Training progress:  28%|██▊       | 11/40 [00:07<00:17,  1.69it/s]\n",
      "Training progress:  30%|███       | 12/40 [00:07<00:16,  1.68it/s]\n",
      "Training progress:  32%|███▎      | 13/40 [00:08<00:15,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoches: 14/40...  Training Steps: 200...  Training Loss: 0.1144...  0.0381 sec/batch\n",
      "\n",
      " Test accuracy: 0.7900... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training progress:  35%|███▌      | 14/40 [00:08<00:15,  1.73it/s]\n",
      "Training progress:  38%|███▊      | 15/40 [00:09<00:14,  1.76it/s]\n",
      "Training progress:  40%|████      | 16/40 [00:09<00:13,  1.77it/s]\n",
      "Training progress:  42%|████▎     | 17/40 [00:10<00:12,  1.80it/s]\n",
      "Training progress:  45%|████▌     | 18/40 [00:11<00:12,  1.82it/s]\n",
      "Training progress:  48%|████▊     | 19/40 [00:11<00:11,  1.79it/s]\n",
      "Training progress:  50%|█████     | 20/40 [00:12<00:11,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoches: 20/40...  Training Steps: 300...  Training Loss: 0.0428...  0.0350 sec/batch\n",
      "\n",
      " Test accuracy: 0.8350... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training progress:  52%|█████▎    | 21/40 [00:12<00:11,  1.73it/s]\n",
      "Training progress:  55%|█████▌    | 22/40 [00:13<00:10,  1.74it/s]\n",
      "Training progress:  57%|█████▊    | 23/40 [00:13<00:09,  1.71it/s]\n",
      "Training progress:  60%|██████    | 24/40 [00:14<00:09,  1.72it/s]\n",
      "Training progress:  62%|██████▎   | 25/40 [00:15<00:08,  1.73it/s]\n",
      "Training progress:  65%|██████▌   | 26/40 [00:15<00:08,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoches: 27/40...  Training Steps: 400...  Training Loss: 0.0438...  0.0387 sec/batch\n",
      "\n",
      " Test accuracy: 0.8200... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training progress:  68%|██████▊   | 27/40 [00:16<00:07,  1.73it/s]\n",
      "Training progress:  70%|███████   | 28/40 [00:16<00:06,  1.75it/s]\n",
      "Training progress:  72%|███████▎  | 29/40 [00:17<00:06,  1.75it/s]\n",
      "Training progress:  75%|███████▌  | 30/40 [00:17<00:05,  1.75it/s]\n",
      "Training progress:  78%|███████▊  | 31/40 [00:18<00:05,  1.75it/s]\n",
      "Training progress:  80%|████████  | 32/40 [00:19<00:04,  1.74it/s]\n",
      "Training progress:  82%|████████▎ | 33/40 [00:19<00:04,  1.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoches: 34/40...  Training Steps: 500...  Training Loss: 0.0872...  0.0559 sec/batch\n",
      "\n",
      " Test accuracy: 0.8200... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training progress:  85%|████████▌ | 34/40 [00:20<00:03,  1.68it/s]\n",
      "Training progress:  88%|████████▊ | 35/40 [00:20<00:02,  1.71it/s]\n",
      "Training progress:  90%|█████████ | 36/40 [00:21<00:02,  1.73it/s]\n",
      "Training progress:  92%|█████████▎| 37/40 [00:22<00:01,  1.74it/s]\n",
      "Training progress:  95%|█████████▌| 38/40 [00:22<00:01,  1.74it/s]\n",
      "Training progress:  98%|█████████▊| 39/40 [00:23<00:00,  1.76it/s]\n",
      "Training progress: 100%|██████████| 40/40 [00:23<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Epoches: 40/40...  Training Steps: 600...  Training Loss: 0.0295...  0.0363 sec/batch\n",
      "\n",
      " Test accuracy: 0.8850... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    counter = 0\n",
    "        \n",
    "    for e in tqdm(range(epochs), desc=\"\\nTraining progress\"):\n",
    "        # Train network\n",
    "        new_state = sess.run(model.initial_state)\n",
    "        loss = 0\n",
    "        for x, y in get_batches(X_train,y_train,n_seqs,n_steps,n_input,n_classes):\n",
    "            counter += 1\n",
    "            start = time.time()\n",
    "            feed = {model.inputs: x,\n",
    "                    model.targets: y,\n",
    "                    model.keep_prob: keep_prob,\n",
    "                    model.initial_state: new_state}\n",
    "            batch_loss, new_state, _ = sess.run([model.loss, \n",
    "                                                 model.final_state, \n",
    "                                                 model.optimizer], \n",
    "                                                 feed_dict=feed)\n",
    "            \n",
    "            end = time.time()\n",
    "            # control the print lines\n",
    "            if counter % 100 == 0:\n",
    "                print('\\n',\n",
    "                      'Epoches: {}/{}... '.format(e+1, epochs),\n",
    "                      'Training Steps: {}... '.format(counter),\n",
    "                      'Training Loss: {:.4f}... '.format(batch_loss),\n",
    "                      '{:.4f} sec/batch'.format((end-start)))\n",
    "                counter2 = 0\n",
    "                test_accuracy = 0\n",
    "                for X_test_lstm, y_test_lstm in get_batches(X_test,y_test,n_seqs,n_steps,n_input,n_classes):\n",
    "                    counter2 += 1\n",
    "                    feed = {model.inputs: X_test_lstm,\n",
    "                            model.targets: y_test_lstm,\n",
    "                            model.keep_prob: 1.,\n",
    "                            model.initial_state: new_state}\n",
    "                    accuracy, = sess.run([model.accuracy], feed_dict=feed)\n",
    "                    test_accuracy += accuracy\n",
    "                test_accuracy = test_accuracy/counter2\n",
    "                print('\\n', 'Test accuracy: {:.4f}... '.format(test_accuracy))\n",
    "\n",
    "    saver.save(sess, \"/home/yxf118/ROS_car/vehicle_data/lstm_model/checkpoints/i{}_l{}.ckpt\".format(counter, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /home/yxf118/ROS_car/vehicle_data/lstm_model/checkpoints/i600_l200.ckpt\n",
      "Test accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "#%% inference\n",
    "\n",
    "checkpoint = tf.train.latest_checkpoint('/home/yxf118/ROS_car/vehicle_data/lstm_model/checkpoints/')\n",
    "model = LSTM_RNN(y_train.shape[1], batch_size=batch_size, num_steps=n_steps,\n",
    "                lstm_size=n_hidden, num_layers=n_layers, learning_rate=learning_rate, \n",
    "                sampling=False)\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # load the model and restoring\n",
    "    saver.restore(sess, checkpoint)\n",
    "    new_state = sess.run(model.initial_state)\n",
    "    \n",
    "    counter = 0\n",
    "    test_accuracy = 0\n",
    "    for X_test_lstm, y_test_lstm in get_batches(X_test,y_test,n_seqs,n_steps,n_input,n_classes):\n",
    "        counter += 1\n",
    "        feed = {model.inputs: X_test_lstm,\n",
    "                model.targets: y_test_lstm,\n",
    "                model.keep_prob: 1.,\n",
    "                model.initial_state: new_state}\n",
    "        accuracy, = sess.run([model.accuracy], feed_dict=feed)\n",
    "        test_accuracy += accuracy\n",
    "    test_accuracy = test_accuracy/counter\n",
    "    print('Test accuracy: {:.4f}'.format(test_accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
